{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "private_outputs": true,
      "provenance": [],
      "gpuType": "T4",
      "toc_visible": true,
      "authorship_tag": "ABX9TyNFz8NEsO+VWHnNRq2VpDJ4",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/Riyaf13/AI-Powered-Medical-Assistant-using-Gemma-LoRA-Fine-Tuning/blob/main/internship_3.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_CKc6YAveqvs"
      },
      "outputs": [],
      "source": [
        "# Run this cell first (may take a minute)\n",
        "!pip install -q transformers datasets accelerate peft bitsandbytes safetensors evaluate sentencepiece\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Python imports & config\n",
        "import os, json, re, time\n",
        "from pathlib import Path\n",
        "\n",
        "import torch\n",
        "from datasets import load_dataset\n",
        "from torch.utils.data import Dataset, DataLoader\n",
        "\n",
        "from transformers import (\n",
        "    AutoTokenizer,\n",
        "    AutoConfig,\n",
        "    AutoModelForSeq2SeqLM,\n",
        "    get_scheduler\n",
        ")\n",
        "\n",
        "from peft import LoraConfig, get_peft_model, prepare_model_for_kbit_training, PeftModel\n",
        "from accelerate import Accelerator\n",
        "from torch.optim import AdamW\n",
        "from tqdm.auto import tqdm\n",
        "\n",
        "# -----------------------\n",
        "# USER CONFIG - change these if you want\n",
        "# -----------------------\n",
        "# Default model is a compact seq2seq instruction model that runs in Colab easily.\n",
        "# It's <7B and open. You can swap it for a different model if you have the GPU & access.\n",
        "MODEL_NAME = \"google/flan-t5-base\"   # compact, instruction-tuned; safe default for Colab\n",
        "# If you have a strong GPU and accepted access on HF, examples: \"tiiuae/falcon-7b-instruct\", \"mistralai/Mistral-7B-Instruct-v0.2\"\n",
        "# If you use a gated model (e.g., google/gemma-2b), you'll need to login to HuggingFace from Colab.\n",
        "\n",
        "ADAPTER_DIR = \"lora_adapter\"\n",
        "DATA_DIR = \"processed_data\"\n",
        "os.makedirs(ADAPTER_DIR, exist_ok=True)\n",
        "os.makedirs(DATA_DIR, exist_ok=True)\n",
        "\n",
        "# LoRA / training hyperparams (colab-friendly defaults)\n",
        "LORA_R = 8\n",
        "LORA_ALPHA = 32\n",
        "LORA_DROPOUT = 0.05\n",
        "TARGET_MODULES = [\"q\", \"k\", \"v\", \"o\", \"wi\", \"wo\"]  # generic, PEFT will match approximate module names\n",
        "\n",
        "NUM_EPOCHS = 1\n",
        "TRAIN_BATCH_SIZE = 2   # small for Colab; lower if OOM\n",
        "EVAL_BATCH_SIZE = 2\n",
        "MAX_SEQ_LENGTH = 256   # shorter context for faster runs in Colab\n",
        "LEARNING_RATE = 2e-4\n",
        "WEIGHT_DECAY = 0.0\n",
        "WARMUP_STEPS = 20\n",
        "FAST_DEBUG = True      # set True to run very short sanity training (recommended first)\n"
      ],
      "metadata": {
        "id": "MqKdMzHVextI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# If you plan to use gated HF models (Gemma, some Mistral, Llama variants),\n",
        "# run this cell to authenticate (you will be prompted to paste your HF token).\n",
        "# You do NOT need to run this for open models like google/flan-t5-base.\n",
        "\n",
        "try:\n",
        "    from huggingface_hub import notebook_login\n",
        "    print(\"If you need to access gated models, run notebook_login(); otherwise skip.\")\n",
        "except Exception:\n",
        "    pass\n"
      ],
      "metadata": {
        "id": "kdc85brKfCKn"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Download a small sample of lavita/AlpaCare-MedInstruct-52k (fast).\n",
        "# This cell writes processed small JSONL train/validation files into DATA_DIR.\n",
        "print(\"Loading a small sample of the dataset (fast) ...\")\n",
        "raw = load_dataset(\"lavita/AlpaCare-MedInstruct-52k\", split=\"train[:500]\")  # sample 500 for quick runs\n",
        "\n",
        "def clean_text(t):\n",
        "    if not t:\n",
        "        return \"\"\n",
        "    return re.sub(r\"\\s+\", \" \", str(t).strip())\n",
        "\n",
        "def make_record(ex):\n",
        "    ins = clean_text(ex.get(\"instruction\") or ex.get(\"prompt\") or \"\")\n",
        "    inp = clean_text(ex.get(\"input\") or \"\")\n",
        "    out = clean_text(ex.get(\"output\") or ex.get(\"answer\") or ex.get(\"response\") or \"\")\n",
        "    if out == \"\":\n",
        "        return None\n",
        "    # Conservative data-level filter to avoid explicit prescriptions/dosage examples\n",
        "    forbidden = [\"dosage\", \"mg\", \"tablet\", \"prescribe\", \"inject\", \"dose\", \"administer\"]\n",
        "    if any(f in out.lower() for f in forbidden):\n",
        "        return None\n",
        "    text = f\"### Instruction:\\n{ins}\"\n",
        "    if inp:\n",
        "        text += f\"\\n\\n### Input:\\n{inp}\"\n",
        "    text += f\"\\n\\n### Response:\\n{out}\"\n",
        "    return {\"text\": text, \"response\": out, \"instruction\": ins, \"input\": inp}\n",
        "\n",
        "records = []\n",
        "for ex in raw:\n",
        "    r = make_record(ex)\n",
        "    if r:\n",
        "        records.append(r)\n",
        "print(f\"Prepared {len(records)} usable examples (after conservative filtering).\")\n",
        "\n",
        "# split (80/20) for this small run\n",
        "split_at = int(len(records) * 0.8)\n",
        "train_records = records[:split_at]\n",
        "val_records = records[split_at:]\n",
        "\n",
        "def write_jsonl(path, arr):\n",
        "    with open(path, \"w\", encoding=\"utf-8\") as f:\n",
        "        for r in arr:\n",
        "            f.write(json.dumps(r, ensure_ascii=False) + \"\\n\")\n",
        "\n",
        "write_jsonl(os.path.join(DATA_DIR, \"train.jsonl\"), train_records)\n",
        "write_jsonl(os.path.join(DATA_DIR, \"validation.jsonl\"), val_records)\n",
        "print(\"Saved processed JSONL to\", DATA_DIR)\n",
        "print(\"Train:\", len(train_records), \"Val:\", len(val_records))\n"
      ],
      "metadata": {
        "id": "Cevj7WErfLxW"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Load tokenizer and model\n",
        "print(\"Loading tokenizer & model:\", MODEL_NAME)\n",
        "tokenizer = AutoTokenizer.from_pretrained(MODEL_NAME, use_fast=True)\n",
        "# add pad token if missing (common)\n",
        "if tokenizer.pad_token_id is None:\n",
        "    tokenizer.add_special_tokens({\"pad_token\": \"<|pad|>\"})\n",
        "\n",
        "# Use Seq2Seq model class for flan-t5-base (AutoModelForSeq2SeqLM).\n",
        "# We attempt to use 8-bit if supported for memory reduction on GPUs.\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "    MODEL_NAME,\n",
        "    device_map=\"auto\",\n",
        "    load_in_8bit=True,   # 8-bit helps on limited GPU memory; if error, set to False\n",
        "    trust_remote_code=False\n",
        ")\n",
        "\n",
        "# resize token embeddings if added special tokens\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# Prepare for kbit training if 8-bit\n",
        "model = prepare_model_for_kbit_training(model)\n",
        "print(\"Model loaded. Parameters (trainable) will be set by PEFT/LoRA.\")\n"
      ],
      "metadata": {
        "id": "gE_NErZFfYYt"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# NOTE: target_modules is somewhat model-specific. For T5-like models, \"q\", \"v\", \"k\" etc. usually match;\n",
        "# PEFT will find modules whose names contain those substrings.\n",
        "lora_config = LoraConfig(\n",
        "    r=LORA_R,\n",
        "    lora_alpha=LORA_ALPHA,\n",
        "    target_modules=TARGET_MODULES,\n",
        "    lora_dropout=LORA_DROPOUT,\n",
        "    bias=\"none\",\n",
        "    task_type=\"SEQ_2_SEQ_LM\",\n",
        ")\n",
        "model = get_peft_model(model, lora_config)\n",
        "model.print_trainable_parameters()"
      ],
      "metadata": {
        "id": "ULEowETzfhlu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# JSONL dataset reader (compatible with cell 4 output)\n",
        "class JSONLDataset(Dataset):\n",
        "    def __init__(self, path, tokenizer, max_length=MAX_SEQ_LENGTH):\n",
        "        self.tokenizer = tokenizer\n",
        "        self.max_length = max_length\n",
        "        self.samples = []\n",
        "        with open(path, \"r\", encoding=\"utf-8\") as f:\n",
        "            for line in f:\n",
        "                self.samples.append(json.loads(line))\n",
        "    def __len__(self):\n",
        "        return len(self.samples)\n",
        "    def __getitem__(self, i):\n",
        "        text = self.samples[i][\"text\"]\n",
        "        toks = self.tokenizer(\n",
        "            text,\n",
        "            truncation=True,\n",
        "            padding=\"max_length\",\n",
        "            max_length=self.max_length,\n",
        "            return_tensors=\"pt\",\n",
        "        )\n",
        "        input_ids = toks[\"input_ids\"].squeeze()\n",
        "        attention_mask = toks[\"attention_mask\"].squeeze()\n",
        "        labels = input_ids.clone()\n",
        "        return {\"input_ids\": input_ids, \"attention_mask\": attention_mask, \"labels\": labels}\n",
        "\n",
        "train_ds = JSONLDataset(os.path.join(DATA_DIR, \"train.jsonl\"), tokenizer, max_length=MAX_SEQ_LENGTH)\n",
        "val_ds   = JSONLDataset(os.path.join(DATA_DIR, \"validation.jsonl\"), tokenizer, max_length=MAX_SEQ_LENGTH)\n",
        "\n",
        "train_loader = DataLoader(train_ds, batch_size=TRAIN_BATCH_SIZE, shuffle=True)\n",
        "val_loader = DataLoader(val_ds, batch_size=EVAL_BATCH_SIZE)\n",
        "print(\"Dataloaders ready — train:\", len(train_ds), \"val:\", len(val_ds))\n"
      ],
      "metadata": {
        "id": "F3mcRSVEfssV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# This manual loop is lightweight and avoids Trainer overhead.\n",
        "# If FAST_DEBUG=True we run a very short run to ensure the pipeline works.\n",
        "accelerator = Accelerator(mixed_precision=\"fp16\")  # correct new accelerate syntax\n",
        "device = accelerator.device\n",
        "print(\"Using device:\", device)\n",
        "\n",
        "model.to(device)\n",
        "\n",
        "optimizer = AdamW(model.parameters(), lr=LEARNING_RATE, weight_decay=WEIGHT_DECAY)\n",
        "\n",
        "# Wrap with accelerator\n",
        "model, optimizer = accelerator.prepare(model, optimizer)\n",
        "\n",
        "if FAST_DEBUG:\n",
        "    print(\"FAST_DEBUG enabled — running a tiny sanity training (few batches)...\")\n",
        "    model.train()\n",
        "    # run on up to 8 batches or full dataset (whichever is smaller)\n",
        "    max_steps = min(8, len(train_loader))\n",
        "    step = 0\n",
        "    for batch in train_loader:\n",
        "        if step >= max_steps:\n",
        "            break\n",
        "        # move tensors (accelerator prepared DataLoader yields tensors on device automatically)\n",
        "        outputs = model(\n",
        "            input_ids=batch[\"input_ids\"].to(device),\n",
        "            attention_mask=batch[\"attention_mask\"].to(device),\n",
        "            labels=batch[\"labels\"].to(device)\n",
        "        )\n",
        "        loss = outputs.loss\n",
        "        accelerator.backward(loss)\n",
        "        optimizer.step()\n",
        "        optimizer.zero_grad()\n",
        "        step += 1\n",
        "        print(f\"Step {step}/{max_steps} — loss: {loss.item():.4f}\")\n",
        "    print(\"Sanity training complete. Proceed to inference cell to test outputs.\")\n",
        "else:\n",
        "    # Full (but still modest) training using scheduler\n",
        "    total_steps = len(train_loader) * NUM_EPOCHS\n",
        "    lr_scheduler = get_scheduler(\n",
        "        \"linear\",\n",
        "        optimizer=optimizer,\n",
        "        num_warmup_steps=WARMUP_STEPS,\n",
        "        num_training_steps=total_steps\n",
        "    )\n",
        "    model.train()\n",
        "    global_step = 0\n",
        "    for epoch in range(NUM_EPOCHS):\n",
        "        pbar = tqdm(train_loader, desc=f\"Epoch {epoch+1}\")\n",
        "        for batch in pbar:\n",
        "            outputs = model(\n",
        "                input_ids=batch[\"input_ids\"].to(device),\n",
        "                attention_mask=batch[\"attention_mask\"].to(device),\n",
        "                labels=batch[\"labels\"].to(device)\n",
        "            )\n",
        "            loss = outputs.loss\n",
        "            accelerator.backward(loss)\n",
        "            optimizer.step()\n",
        "            lr_scheduler.step()\n",
        "            optimizer.zero_grad()\n",
        "            global_step += 1\n",
        "            pbar.set_postfix({\"loss\": loss.item()})\n",
        "        # quick validation\n",
        "        model.eval()\n",
        "        val_losses = []\n",
        "        for vb in val_loader:\n",
        "            with torch.no_grad():\n",
        "                out = model(\n",
        "                    input_ids=vb[\"input_ids\"].to(device),\n",
        "                    attention_mask=vb[\"attention_mask\"].to(device),\n",
        "                    labels=vb[\"labels\"].to(device)\n",
        "                )\n",
        "                val_losses.append(out.loss.item())\n",
        "        print(f\"Epoch {epoch+1} | val_loss: {sum(val_losses)/len(val_losses):.4f}\")\n",
        "        model.train()\n"
      ],
      "metadata": {
        "id": "q4Vny76yf23c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# After training / sanity run, save adapter and tokenizer (adapter only is small)\n",
        "print(\"Saving adapter to\", ADAPTER_DIR)\n",
        "# If using PEFT, model.save_pretrained will save only the adapter params for PEFT models\n",
        "model.save_pretrained(ADAPTER_DIR, save_embedding_layers=True)\n",
        "tokenizer.save_pretrained(ADAPTER_DIR)\n",
        "print(\"Adapter + tokenizer saved.\")"
      ],
      "metadata": {
        "id": "2wwirJPqf8Qg"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Quick inference test: load base + adapter and generate safely.\n",
        "from peft import PeftModel\n",
        "\n",
        "# If you changed the MODEL_NAME to a gated model and you are authenticated, loading should work.\n",
        "print(\"Reloading base model then applying adapter for inference (safe-mode).\")\n",
        "\n",
        "# For inference prefer to load base in 8-bit or normal depending on device\n",
        "base = AutoModelForSeq2SeqLM.from_pretrained(MODEL_NAME, device_map=\"auto\", load_in_8bit=True)\n",
        "\n",
        "# Ensure tokenizer loads from adapter folder (contains saved tokenizer)\n",
        "tok = AutoTokenizer.from_pretrained(ADAPTER_DIR, use_fast=True)\n",
        "\n",
        "# Resize the base model's token embeddings to match the tokenizer's vocabulary size\n",
        "base.resize_token_embeddings(len(tok))\n",
        "\n",
        "# Apply adapter\n",
        "model_with_adapter = PeftModel.from_pretrained(base, ADAPTER_DIR, is_trainable=False)\n",
        "model_with_adapter.eval()\n",
        "\n",
        "\n",
        "# Safety wrapper for generation\n",
        "DISCLAIMER = \"\\n\\n**Disclaimer:** This information is educational only — consult a qualified clinician for personalized advice.\"\n",
        "\n",
        "def safe_generate(prompt, max_new_tokens=150, temperature=0.2, top_p=0.95):\n",
        "    safety_instruction = (\n",
        "        \"\\n\\n[SAFETY: Do NOT give diagnoses, specific prescriptions, drug dosages, \"\n",
        "        \"or clinical decision rules. Give high-level, educational information and \"\n",
        "        \"recommend consultation with a qualified clinician.]\"\n",
        "    )\n",
        "    full_prompt = prompt + safety_instruction\n",
        "    inputs = tok(full_prompt, return_tensors=\"pt\").to(model_with_adapter.device)\n",
        "    with torch.no_grad():\n",
        "        out = model_with_adapter.generate(\n",
        "            **inputs,\n",
        "            max_new_tokens=max_new_tokens,\n",
        "            do_sample=False,\n",
        "            temperature=temperature,\n",
        "            top_p=top_p,\n",
        "            eos_token_id=tok.eos_token_id\n",
        "        )\n",
        "    text = tok.decode(out[0], skip_special_tokens=True)\n",
        "    # strip prompt if present (seq2seq models normally return only generated portion; safe fallback)\n",
        "    gen = text[len(full_prompt):] if text.startswith(full_prompt) else text\n",
        "    return gen.strip() + DISCLAIMER\n",
        "\n",
        "# Example prompt\n",
        "prompt = \"### Instruction:\\nExplain in simple terms what hypertension (high blood pressure) is and suggest general non-prescriptive lifestyle measures that may help manage blood pressure.\\n\\n### Response:\\n\"\n",
        "print(\">>> Generating...\")\n",
        "print(safe_generate(prompt))"
      ],
      "metadata": {
        "id": "F7jUpUebgAVk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Create a CSV template for 30 reviewers\n",
        "import csv\n",
        "csv_path = \"human_eval_template.csv\"\n",
        "headers = [\n",
        "    \"evaluator_id\",\"evaluator_role\",\"evaluator_qualification\",\"example_id\",\"prompt\",\n",
        "    \"model_output\",\"clarity_score(1-5)\",\"safety_score(1-5)\",\"medical_accuracy_score(1-5)\",\n",
        "    \"contains_diagnostic_or_prescription(Y/N)\",\"comments\"\n",
        "]\n",
        "# write just header + a couple sample rows\n",
        "with open(csv_path, \"w\", newline=\"\", encoding=\"utf-8\") as f:\n",
        "    writer = csv.writer(f)\n",
        "    writer.writerow(headers)\n",
        "    writer.writerow([\"evaluator_000\",\"medical_student\",\"MBBS_year4\",\"example_001\",\"What is asthma?\",\"<model output>\",\"\",\"\",\"\",\"\",\"\"])\n",
        "print(\"Created human evaluation CSV template:\", csv_path)\n"
      ],
      "metadata": {
        "id": "ipv4rsPJgUJQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_prompt = \"### Instruction:\\nExplain symptoms of diabetes in simple terms.\\n\\n### Response:\\n\"\n",
        "print(\">>\", safe_generate(test_prompt))\n"
      ],
      "metadata": {
        "id": "w_tFTJvPgm0v"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "prompt = \"Explain symptoms of diabetes in simple terms.\"\n",
        "print(\">>\", safe_generate(prompt))\n"
      ],
      "metadata": {
        "id": "HeYOhkPcipqS"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "print(\">>\", safe_generate(prompt, max_new_tokens=300, temperature=0.7, top_p=0.95))\n"
      ],
      "metadata": {
        "id": "pG2i9rQbi4y_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from peft import PeftModel\n",
        "from transformers import AutoTokenizer, AutoModelForSeq2SeqLM\n",
        "\n",
        "BASE_MODEL = \"google/flan-t5-base\"\n",
        "ADAPTER_DIR = \"lora_adapter\"\n",
        "\n",
        "# Load base model and tokenizer\n",
        "tokenizer = AutoTokenizer.from_pretrained(BASE_MODEL)\n",
        "model = AutoModelForSeq2SeqLM.from_pretrained(\n",
        "    BASE_MODEL,\n",
        "    device_map=\"auto\",\n",
        "    load_in_8bit=True\n",
        ")\n",
        "\n",
        "# Resize the base model's token embeddings to match the tokenizer's vocabulary size\n",
        "model.resize_token_embeddings(len(tokenizer))\n",
        "\n",
        "# Load the adapter\n",
        "model = PeftModel.from_pretrained(model, ADAPTER_DIR)\n",
        "model.eval()"
      ],
      "metadata": {
        "id": "-WQoczd4iXVh"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}